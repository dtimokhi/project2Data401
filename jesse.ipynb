{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from scipy.sparse.linalg import svds, eigs\n",
    "from sklearn import datasets\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    \n",
    "    def __call__(self, predicted, actual):\n",
    "        \"\"\"Calculates the loss as a function of the prediction and the actual.\n",
    "        \n",
    "        Args:\n",
    "          predicted (np.ndarray, float): the predicted output labels\n",
    "          actual (np.ndarray, float): the actual output labels\n",
    "          \n",
    "        Returns: (float) \n",
    "          The value of the loss for this batch of observations.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def derivative(self, predicted, actual):\n",
    "        \"\"\"The derivative of the loss with respect to the prediction.\n",
    "        \n",
    "        Args:\n",
    "          predicted (np.ndarray, float): the predicted output labels\n",
    "          actual (np.ndarray, float): the actual output labels\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          The derivatives of the loss.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "              \n",
    "class SquaredErrorLoss(Loss):\n",
    "    #CHANGE THIS TO ** rather than np.square?\n",
    "    def __call__(self, predicted, actual):\n",
    "        return predicted-actual**2\n",
    "    def derivative(self, predicted, actual):\n",
    "        return 2*np.subtract(predicted, actual)\n",
    "    \n",
    "class ActivationFunction(object):\n",
    "        \n",
    "    def __call__(self, a):\n",
    "        \"\"\"Applies activation function to the values in a layer.\n",
    "        \n",
    "        Args:\n",
    "          a (np.ndarray, float): the values from the previous layer (after \n",
    "            multiplying by the weights.\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          The values h = g(a).\n",
    "        \"\"\"\n",
    "        return a\n",
    "    \n",
    "    def derivative(self, h):\n",
    "        \"\"\"The derivatives as a function of the outputs at the nodes.\n",
    "        \n",
    "        Args:\n",
    "          h (np.ndarray, float): the outputs h = g(a) at the nodes.\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          The derivatives dh/da.\n",
    "        \"\"\"\n",
    "        return 1\n",
    "       \n",
    "class ReLU(ActivationFunction):\n",
    "    def __call__(self, a):\n",
    "        return np.clip(a, 0, None)\n",
    "    def derivative(self, h):\n",
    "        return np.clip(h, 0, 1)\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    def __call__(self, a):\n",
    "        return 1.0/(1.0+np.exp(-a))\n",
    "    def derivative(self, h):\n",
    "        return self.__call__(h)*(1-self.__call__(h))\n",
    "    \n",
    "class Layer(object):\n",
    "    \"\"\"A data structure for a layer in a neural network.\n",
    "    \n",
    "    Attributes:\n",
    "      num_nodes (int): number of nodes in the layer\n",
    "      activation_function (ActivationFunction)\n",
    "      values_pre_activation (np.ndarray, float): most recent values\n",
    "        in layer, before applying activation function\n",
    "      values_post_activation (np.ndarray, float): most recent values\n",
    "        in layer, after applying activation function\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_nodes, activation_function=ActivationFunction()):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "    def get_layer_values(self, values_pre_activation):\n",
    "        \"\"\"Applies activation function to values from previous layer.\n",
    "        \n",
    "        Stores the values (both before and after applying activation \n",
    "        function)\n",
    "        \n",
    "        Args:\n",
    "          values_pre_activation (np.ndarray, float): \n",
    "            A (batch size) x self.num_nodes array of the values\n",
    "            in layer before applying the activation function\n",
    "        \n",
    "        Returns: (np.ndarray, float)\n",
    "            A (batch size) x self.num_nodes array of the values\n",
    "            in layer after applying the activation function\n",
    "        \"\"\"\n",
    "        self.values_pre_activation = values_pre_activation\n",
    "        self.values_post_activation = self.activation_function(\n",
    "            values_pre_activation\n",
    "        )\n",
    "        return self.values_post_activation\n",
    "\n",
    "        \n",
    "class FullyConnectedNeuralNetwork(object):\n",
    "    \"\"\"A data structure for a fully-connected neural network.\n",
    "    \n",
    "    Attributes:\n",
    "      layers (Layer): A list of Layer objects.\n",
    "      loss (Loss): The loss function to use in training.\n",
    "      learning_rate (float): The learning rate to use in backpropagation.\n",
    "      weights (list, np.ndarray): A list of weight matrices,\n",
    "        length should be len(self.layers) - 1\n",
    "      biases (list, float): A list of bias terms,\n",
    "        length should be equal to len(self.layers)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layers, loss, learning_rate):\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # initialize weight matrices and biases to zeros\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.weights.append(\n",
    "                np.random.normal(0, 1, (self.layers[i - 1].num_nodes, self.layers[i].num_nodes))\n",
    "            )\n",
    "            self.biases.append(\n",
    "                np.zeros(self.layers[i].num_nodes)\n",
    "            )\n",
    "    \n",
    "    def feedforward(self, inputs):\n",
    "        \"\"\"Predicts the output(s) for a given set of input(s).\n",
    "        \n",
    "        Args:\n",
    "          inputs (np.ndarray, float): A (batch size) x self.layers[0].num_nodes array\n",
    "          \n",
    "        Returns: (np.ndarray, float) \n",
    "          An array of the predicted output labels, length is the batch size\n",
    "        \"\"\"\n",
    "        # TODO: Implement feedforward prediction.\n",
    "        # Make sure you use Layer.get_layer_values() at each layer to store the values\n",
    "        # for later use in backpropagation.\n",
    "\n",
    "        h = self.layers[0].get_layer_values(inputs)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            b = self.biases[i-1]\n",
    "            w = self.weights[i-1]\n",
    "            z = np.matmul(h, w) + b\n",
    "            h = self.layers[i].get_layer_values(z)\n",
    "        return h\n",
    "        \n",
    "    def backprop(self, predicted, actual):\n",
    "        \"\"\"Updates self.weights and self.biases based on predicted and actual values.\n",
    "        \n",
    "        This will require using the values at each layer that were stored at the\n",
    "        feedforward step.\n",
    "        \n",
    "        Args:\n",
    "          predicted (np.ndarray, float): An array of the predicted output labels\n",
    "          actual (np.ndarray, float): An array of the actual output labels\n",
    "        \"\"\"\n",
    "        \n",
    "        w_new = [np.zeros(w.shape) for w in self.weights]\n",
    "        b_new = [np.zeros(b.shape) for b in self.biases]\n",
    "        n = len(predicted)\n",
    "        if(n == 1):\n",
    "            delta = self.loss.derivative(predicted, actual)\n",
    "            b_new[-1] = b_new[-1] + self.learning_rate * delta.T\n",
    "            w_new[-1] = w_new[-1] + self.learning_rate * np.dot(delta, self.layers[-2].values_post_activation).T\n",
    "            for i in range(2, len(self.layers)):\n",
    "                a = (self.layers[-i].values_pre_activation)\n",
    "                h = (self.layers[-i-1].values_post_activation)\n",
    "                g_prime = self.layers[-i].activation_function.derivative(a)\n",
    "                delta = np.multiply(np.dot(self.weights[-i+1], delta), g_prime.T)\n",
    "                b_new[-i] = b_new[-i] + self.learning_rate * delta.T\n",
    "                w_new[-i] = w_new[-i] + self.learning_rate * np.dot(delta, h).T\n",
    "\n",
    "            self.weights = [np.subtract(x, y) for x, y in zip(self.weights, w_new)]\n",
    "            self.biases = [np.subtract(x, y) for x, y in zip(self.biases, b_new)]\n",
    "        else:\n",
    "            for j in range(n):\n",
    "                delta = np.array([self.loss.derivative(predicted[j], actual[j])])\n",
    "                b_new[-1] = b_new[-1] + self.learning_rate * delta.T\n",
    "                w_new[-1] = w_new[-1] + self.learning_rate * np.dot(delta, [self.layers[-2].values_post_activation[j]]).T\n",
    "                for i in range(2, len(self.layers)):\n",
    "                    a = [(self.layers[-i].values_pre_activation)[j]]\n",
    "                    h = [(self.layers[-i-1].values_post_activation)[j]]\n",
    "                    g_prime = self.layers[-i].activation_function.derivative(a)\n",
    "                    delta = np.multiply(np.dot(self.weights[-i+1], delta), g_prime.T)\n",
    "                    b_new[-i] = b_new[-i] + self.learning_rate * delta.T\n",
    "                    w_new[-i] = w_new[-i] + self.learning_rate * np.dot(delta, h).T\n",
    "\n",
    "            self.weights = [np.subtract(x, y/n) for x, y in zip(self.weights, w_new)]\n",
    "            self.biases = [np.subtract(x, y/n) for x, y in zip(self.biases, b_new)]\n",
    "\n",
    "        \n",
    "    def train(self, inputs, labels):\n",
    "        \"\"\"Trains neural network based on a batch of training data.\n",
    "        \n",
    "        Args:\n",
    "          inputs (np.ndarray): A (batch size) x self.layers[0].num_nodes array\n",
    "          labels (np.ndarray): An array of ground-truth output labels, \n",
    "            length is the batch size.\n",
    "        \"\"\"\n",
    "        predicted = self.feedforward(inputs)\n",
    "#         print(predicted)\n",
    "        self.backprop(predicted, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer = pd.read_csv(\"full_data.csv\", index_col=0)\n",
    "beer.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beer/ABV</th>\n",
       "      <th>review/appearance</th>\n",
       "      <th>review/aroma</th>\n",
       "      <th>review/overall</th>\n",
       "      <th>review/palate</th>\n",
       "      <th>review/taste</th>\n",
       "      <th>review/text</th>\n",
       "      <th>isdst</th>\n",
       "      <th>\"The Wind Cried Mari...\" Scottish Heather Ale</th>\n",
       "      <th>1906 Reserva Especial</th>\n",
       "      <th>...</th>\n",
       "      <th>2.0.3</th>\n",
       "      <th>3.0.3</th>\n",
       "      <th>4.0.3</th>\n",
       "      <th>5.0.3</th>\n",
       "      <th>6.0.3</th>\n",
       "      <th>avg_palate</th>\n",
       "      <th>avg_aroma</th>\n",
       "      <th>avg_overall</th>\n",
       "      <th>avg_taste</th>\n",
       "      <th>avg_appear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Pours a clouded gold with a thin white head. N...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12oz bottle into 8oz snifter.\\t\\tDeep ruby red...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>3.972222</td>\n",
       "      <td>3.611111</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>3.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>First enjoyed at the brewpub about 2 years ago...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.653846</td>\n",
       "      <td>3.461538</td>\n",
       "      <td>3.903846</td>\n",
       "      <td>3.769231</td>\n",
       "      <td>3.711538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>First thing I noticed after pouring from green...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.602564</td>\n",
       "      <td>3.431090</td>\n",
       "      <td>3.820513</td>\n",
       "      <td>3.661859</td>\n",
       "      <td>3.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>A: pours an amber with a one finger head but o...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.792135</td>\n",
       "      <td>3.646067</td>\n",
       "      <td>3.848315</td>\n",
       "      <td>3.758427</td>\n",
       "      <td>3.904494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 802 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   beer/ABV  review/appearance  review/aroma  review/overall  review/palate  \\\n",
       "0       5.0                4.0           4.0             4.0            4.0   \n",
       "1      11.0                4.0           3.5             3.5            3.5   \n",
       "2       4.7                3.5           4.0             3.5            3.5   \n",
       "3       4.4                3.0           3.0             2.5            3.0   \n",
       "4       4.4                4.0           3.0             3.0            3.5   \n",
       "\n",
       "   review/taste                                        review/text  isdst  \\\n",
       "0           4.0  Pours a clouded gold with a thin white head. N...    0.0   \n",
       "1           3.0  12oz bottle into 8oz snifter.\\t\\tDeep ruby red...    0.0   \n",
       "2           3.5  First enjoyed at the brewpub about 2 years ago...    0.0   \n",
       "3           3.0  First thing I noticed after pouring from green...    0.0   \n",
       "4           2.5  A: pours an amber with a one finger head but o...    0.0   \n",
       "\n",
       "   \"The Wind Cried Mari...\" Scottish Heather Ale  1906 Reserva Especial  \\\n",
       "0                                              0                      0   \n",
       "1                                              0                      0   \n",
       "2                                              0                      0   \n",
       "3                                              0                      0   \n",
       "4                                              0                      0   \n",
       "\n",
       "      ...      2.0.3  3.0.3  4.0.3  5.0.3  6.0.3  avg_palate  avg_aroma  \\\n",
       "0     ...          0      0      0      0      0    3.166667   3.833333   \n",
       "1     ...          0      0      1      0      0    3.916667   3.972222   \n",
       "2     ...          0      0      1      0      0    3.653846   3.461538   \n",
       "3     ...          0      0      0      0      0    3.602564   3.431090   \n",
       "4     ...          0      0      0      1      0    3.792135   3.646067   \n",
       "\n",
       "   avg_overall  avg_taste  avg_appear  \n",
       "0     3.166667   3.000000    3.666667  \n",
       "1     3.611111   3.833333    3.888889  \n",
       "2     3.903846   3.769231    3.711538  \n",
       "3     3.820513   3.661859    3.692308  \n",
       "4     3.848315   3.758427    3.904494  \n",
       "\n",
       "[5 rows x 802 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = EnglishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "stem_vectorizer = CountVectorizer(\n",
    "    stop_words = 'english',\n",
    "    analyzer=stemmed_words,\n",
    "    ngram_range = (1,2)\n",
    ")\n",
    "tfidf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorize the reviews\n",
    "X_train_counts = stem_vectorizer.fit_transform(beer['review/text'])\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, v = svds(X_train_tfidf, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.concat([beer, pd.DataFrame(u)], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.drop('review/text', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = full_data.drop(['review/appearance', 'review/aroma', 'review/overall', 'review/palate', 'review/taste'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37494, 896)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = full_data['review/overall'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.to_csv('final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beer/ABV</th>\n",
       "      <th>review/appearance</th>\n",
       "      <th>review/aroma</th>\n",
       "      <th>review/overall</th>\n",
       "      <th>review/palate</th>\n",
       "      <th>review/taste</th>\n",
       "      <th>isdst</th>\n",
       "      <th>\"The Wind Cried Mari...\" Scottish Heather Ale</th>\n",
       "      <th>1906 Reserva Especial</th>\n",
       "      <th>2X Chocolate Porter</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003817</td>\n",
       "      <td>-0.001904</td>\n",
       "      <td>-0.002556</td>\n",
       "      <td>0.003672</td>\n",
       "      <td>-0.000728</td>\n",
       "      <td>-0.002721</td>\n",
       "      <td>-0.003397</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.003669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008872</td>\n",
       "      <td>-0.008459</td>\n",
       "      <td>-0.005390</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.003362</td>\n",
       "      <td>-0.000566</td>\n",
       "      <td>-0.001022</td>\n",
       "      <td>-0.000789</td>\n",
       "      <td>-0.004414</td>\n",
       "      <td>0.005061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003155</td>\n",
       "      <td>-0.001823</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.002276</td>\n",
       "      <td>-0.000769</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>0.004495</td>\n",
       "      <td>-0.000685</td>\n",
       "      <td>-0.007607</td>\n",
       "      <td>0.003911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001222</td>\n",
       "      <td>0.002758</td>\n",
       "      <td>-0.001472</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.008042</td>\n",
       "      <td>0.012155</td>\n",
       "      <td>0.009179</td>\n",
       "      <td>0.004690</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>0.003160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003644</td>\n",
       "      <td>-0.009948</td>\n",
       "      <td>-0.001942</td>\n",
       "      <td>0.005490</td>\n",
       "      <td>0.003194</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.004289</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>-0.002551</td>\n",
       "      <td>0.005537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 901 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   beer/ABV  review/appearance  review/aroma  review/overall  review/palate  \\\n",
       "0       5.0                4.0           4.0             4.0            4.0   \n",
       "1      11.0                4.0           3.5             3.5            3.5   \n",
       "2       4.7                3.5           4.0             3.5            3.5   \n",
       "3       4.4                3.0           3.0             2.5            3.0   \n",
       "4       4.4                4.0           3.0             3.0            3.5   \n",
       "\n",
       "   review/taste  isdst  \"The Wind Cried Mari...\" Scottish Heather Ale  \\\n",
       "0           4.0    0.0                                            0.0   \n",
       "1           3.0    0.0                                            0.0   \n",
       "2           3.5    0.0                                            0.0   \n",
       "3           3.0    0.0                                            0.0   \n",
       "4           2.5    0.0                                            0.0   \n",
       "\n",
       "   1906 Reserva Especial  2X Chocolate Porter    ...           90        91  \\\n",
       "0                    0.0                  0.0    ...    -0.003817 -0.001904   \n",
       "1                    0.0                  0.0    ...     0.008872 -0.008459   \n",
       "2                    0.0                  0.0    ...    -0.003155 -0.001823   \n",
       "3                    0.0                  0.0    ...    -0.001222  0.002758   \n",
       "4                    0.0                  0.0    ...     0.003644 -0.009948   \n",
       "\n",
       "         92        93        94        95        96        97        98  \\\n",
       "0 -0.002556  0.003672 -0.000728 -0.002721 -0.003397  0.003552  0.000400   \n",
       "1 -0.005390  0.001952  0.003362 -0.000566 -0.001022 -0.000789 -0.004414   \n",
       "2  0.002242  0.002276 -0.000769  0.002075  0.004495 -0.000685 -0.007607   \n",
       "3 -0.001472  0.000550  0.008042  0.012155  0.009179  0.004690  0.001785   \n",
       "4 -0.001942  0.005490  0.003194  0.000178  0.004289  0.001215 -0.002551   \n",
       "\n",
       "         99  \n",
       "0  0.003669  \n",
       "1  0.005061  \n",
       "2  0.003911  \n",
       "3  0.003160  \n",
       "4  0.005537  \n",
       "\n",
       "[5 rows x 901 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7dbb6190b727>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfull_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'full_data' is not defined"
     ]
    }
   ],
   "source": [
    "full_data[full_data.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [Layer(250)] + ([Layer(20, ReLU())] * 10) + [Layer(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_1 = FullyConnectedNeuralNetwork(\n",
    "    layers=l,\n",
    "    loss = SquaredErrorLoss(),\n",
    "    learning_rate= 0.00000000000000000001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10\n",
    "for i in range(0, len(y_train), n):\n",
    "    network_1.train(X_train[i:i+n], y_train[i:i+n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-684.74625613]])"
      ]
     },
     "execution_count": 852,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_1.feedforward([X_test[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = network_1.feedforward(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1774.98918098],\n",
       "       [-2845.03400576],\n",
       "       [-2480.15262322],\n",
       "       ..., \n",
       "       [-1133.30312844],\n",
       "       [-5720.87303684],\n",
       "       [-7483.86534858]])"
      ]
     },
     "execution_count": 854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-21600.749241491692, 121.08819412854741)"
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.min(), predicted.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13301956.922905698"
      ]
     },
     "execution_count": 856,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((y_test - predicted.flatten())**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.40000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "         -1.57359374e-03,  -4.17524945e-03,   6.08118718e-03],\n",
       "       [  1.12000000e+01,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "         -5.61188148e-03,   9.76825454e-03,   6.26696735e-03],\n",
       "       [  5.90000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "         -1.32121412e-04,  -6.73356530e-03,   2.80870574e-03],\n",
       "       ..., \n",
       "       [  4.40000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "         -3.83761954e-03,  -1.08731993e-02,   4.39019528e-03],\n",
       "       [  1.05000000e+01,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "         -4.31864509e-03,  -5.39410540e-03,   4.79386121e-03],\n",
       "       [  1.20000000e+01,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          2.14395446e-03,  -2.11244398e-03,   4.37316158e-03]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = (y_train - np.mean(y_train))/(np.std(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (X_train - np.mean(X_train))/(np.std(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.60000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "         -5.43027851e-03,   4.53009133e-03,   3.44912530e-03],\n",
       "       [  8.50000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "         -4.24265087e-04,  -1.75124991e-03,   3.06964660e-03],\n",
       "       [  8.30000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "         -8.63554913e-03,   5.45475078e-03,   5.04183697e-03],\n",
       "       ..., \n",
       "       [  6.30000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "         -6.23733596e-04,  -1.26662604e-03,   4.72117017e-03],\n",
       "       [  1.20000000e+01,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          3.20949235e-03,   3.91589245e-03,   5.28097794e-03],\n",
       "       [  7.20000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "         -9.33489452e-04,  -1.09086857e-02,   5.34232776e-03]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "28120/28120 [==============================] - 2s 55us/step - loss: nan\n",
      "Epoch 2/5\n",
      "28120/28120 [==============================] - 1s 51us/step - loss: nan\n",
      "Epoch 3/5\n",
      "28120/28120 [==============================] - 1s 46us/step - loss: nan\n",
      "Epoch 4/5\n",
      "28120/28120 [==============================] - 1s 45us/step - loss: nan\n",
      "Epoch 5/5\n",
      "28120/28120 [==============================] - 1s 45us/step - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f96dd323ef0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify the model architecture\n",
    "# l = [layers.Dense(896, activation=\"relu\")] + ([layers.Dense(20, activation=\"relu\")] * 20) + [layers.Dense(1)]\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(896, activation=\"relu\"),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# specify the loss function and optimization function\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(0.000000000000001),\n",
    "              loss='mse')\n",
    "\n",
    "# fit the model to data\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ nan],\n",
       "       [ nan],\n",
       "       [ nan],\n",
       "       ..., \n",
       "       [ nan],\n",
       "       [ nan],\n",
       "       [ nan]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = FullyConnectedNeuralNetwork(\n",
    "    layers=[Layer(10), Layer(20, ReLU()), Layer(1)],\n",
    "    loss = SquaredErrorLoss(),\n",
    "    learning_rate=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y)):\n",
    "    network.train([X[i]], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8344.7284204534008"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((y - network.feedforward(X))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
